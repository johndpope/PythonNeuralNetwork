{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neurons: The Basics Of Neural Networks\n",
    "This notebook is where I explore neural networks on a much more basic and fundamental level. Instead of focusing on creating a NN that can solve the universe, I want to fully aquaint myself with the fundamentals of linear algebra in ML, backpropogation and optimisation techniques. That's what this notebook is aimed to do: simply learn the basics in a much more controlled way.\n",
    "\n",
    "I'll be using a simple case of three input neurons, _no_ hidden neurons and a single output neuron. I've designed the system this way since I want to attempt to \"learn\" the behaviour of an [AND](https://en.wikipedia.org/wiki/AND_gate) gate and an [OR](https://en.wikipedia.org/wiki/OR_gate) gate.\n",
    "\n",
    "## Acknowledgements\n",
    "Once again, I'm using a plethora of material to learn about NN and how to implement them. In no particular order, here are my main reads that have helped me massively.\n",
    " - iamtrask's blog article on \"[A Neural Network in 11 lines of Python](https://iamtrask.github.io/2015/07/12/basic-python-network/)\"\n",
    " - Matt Mazur's fantastic article on the theory behind backpropogation, \"[A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\" \n",
    " - Analytics Vidhya's article on \"[Understanding and coding Neural Networks From Scratch in Python and R](https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing the Neural Network\n",
    "Before we can actually do anything, we need to develop the neural network that we will be using. Fortunately I already (roughly) know how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the important packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define any globals.\n",
    "np.random.seed(0)\n",
    "DATA_POINTS = 10000  # 10 Thousand\n",
    "INPUT_NEURONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the important functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Normalises the inputs to be between 0 and 1\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def S(x):\n",
    "    \"\"\"Shorthand notation for the sigmoid function\"\"\"\n",
    "    return sigmoid(x)\n",
    "\n",
    "def dS(x_norm):\n",
    "    \"\"\"Shorthand notation for the derivative of the sigmoid function\"\"\"\n",
    "    return x_norm*(1-x_norm)\n",
    "\n",
    "def errorFunc(expected, actual):\n",
    "    \"\"\"Calculates the error\"\"\"\n",
    "    return 0.5*(expected - actual)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the NN class.\n",
    "class NeuralNetwork(object):\n",
    "    \"\"\"The neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self, x, y, seed=None):\n",
    "        \"\"\"Initialise the NN\"\"\"        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        if seed: np.random.seed(0)\n",
    "        \n",
    "        self.weights = 2*np.random.random((3,1)) - 1\n",
    "        self.bias = 2*np.random.random((1, 1)) - 1\n",
    "        return None\n",
    "    \n",
    "    def train(self, epochs=1, verbose=False):\n",
    "        \"\"\"Trains the network for the given iterations\"\"\"        \n",
    "        for i in xrange(epochs):  # Go over the data N times\n",
    "            if verbose: print \"%i/%i epochs complete\" % (i, epochs)\n",
    "            for x, y in zip(self.x, self.y):  # The data itself (to avoid matrix mechanics)\n",
    "                # Forward propagation\n",
    "                output = S(np.dot(x.T, self.weights) + self.bias[0])\n",
    "\n",
    "                # Calculate error\n",
    "                error = errorFunc(y, output)\n",
    "\n",
    "                # Backward propagate weights\n",
    "                learning_rate = error\n",
    "                delta = dS(output)*(output-y)\n",
    "                self.weights = self.weights + np.array([(-delta*learning_rate)[0]*x]).T\n",
    "                self.bias = self.bias - [delta*learning_rate]  # Since all bias \"inputs\" are 1\n",
    "        \n",
    "        if verbose:\n",
    "            print \"Weights:\", list(self.weights)\n",
    "            print \"Bias:\", list(self.bias)\n",
    "        return None\n",
    "    \n",
    "    def think(self, x):\n",
    "        \"\"\"Given the inputs x, let the NN predict y.\"\"\"\n",
    "        # Forward propagation\n",
    "        input_layer = x\n",
    "        output_layer = S(np.dot(input_layer, self.weights) + self.bias[0])\n",
    "        return output_layer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - The AND gate\n",
    "The first problem I'll be attempting to solve is for the AND gate. This gate takes boolean inputs (True or False) and outputs True if all inputs are True or False if any inputs are False. To express this in Boolean algebra:\n",
    "\n",
    "> AND(A, B, C) = ABC\n",
    "\n",
    "where the inputs can only be 0 or 1. Intuatively I expect that, without normalisation, the weights $w_1$, $w_2$ and $w_3$ will all be 0.33 and the bias, $b$, will be -0.16. That's because the weighted summation will then only be above 0.5 (the \"True\" threshold) when all three inputs fire.\n",
    "\n",
    "With this in mind, let's now generate the data for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AND(x, y, z):\n",
    "    return x and y and z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(2, size=(INPUT_NEURONS*DATA_POINTS))\n",
    "x = np.split(x, DATA_POINTS)\n",
    "\n",
    "y = [AND(a, b, c) for a, b, c in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data fully primed and ready, we can now train the neural network and see how good of a result it gets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork(x, y, seed=1)\n",
    "NN.train(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that the model works, we can test it against the various inputs of the AND gate to see what it thinks the answer might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY CHECK\n",
      "==================\n",
      "[0, 0, 0]: 2.41194618326e-06 -> 0\n",
      "[0, 0, 1]: 0.000405165926404 -> 0\n",
      "[0, 1, 0]: 0.000403058006319 -> 0\n",
      "[0, 1, 1]: 0.0634612518911 -> 0\n",
      "[1, 0, 0]: 0.000412239702661 -> 0\n",
      "[1, 0, 1]: 0.0648137640716 -> 0\n",
      "[1, 1, 0]: 0.0644981850056 -> 0\n",
      "[1, 1, 1]: 0.920548253539 -> 1\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "validation_x = [\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 1],\n",
    "]\n",
    "\n",
    "validation_y = [AND(a, b, c) for a, b, c in validation_x]\n",
    "\n",
    "# Now see how accurate it is.\n",
    "count = 0\n",
    "print \"ACCURACY CHECK\"\n",
    "print \"==================\"\n",
    "for x, y in zip(validation_x, validation_y):\n",
    "    prediction = NN.think(x)\n",
    "    print \"%s: %s -> %i\" % (x, prediction, int(round(prediction)))\n",
    "    count += int(int(round(prediction) == y))\n",
    "print \"\\nAccuracy:\", float(count)/len(validation_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - The OR Gate\n",
    "With the AND gate all done and dusted, we can shift our attention to getting the OR gate data to play aroun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
