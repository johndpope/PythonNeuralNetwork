{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neurons: The Basics Of Neural Networks\n",
    "This notebook is where I explore neural networks on a much more basic and fundamental level. Instead of focusing on creating a NN that can solve the universe, I want to fully aquaint myself with the fundamentals of linear algebra in ML, backpropogation and optimisation techniques. That's what this notebook is aimed to do: simply learn the basics in a much more controlled way.\n",
    "\n",
    "I'll be using a simple case of three input neurons, _no_ hidden neurons and a single output neuron. I've designed the system this way since I want to attempt to \"learn\" the behaviour of an [AND](https://en.wikipedia.org/wiki/AND_gate) gate and an [OR](https://en.wikipedia.org/wiki/OR_gate) gate.\n",
    "\n",
    "## Acknowledgements\n",
    "Once again, I'm using a plethora of material to learn about NN and how to implement them. In no particular order, here are my main reads that have helped me massively.\n",
    " - iamtrask's blog article on \"[A Neural Network in 11 lines of Python](https://iamtrask.github.io/2015/07/12/basic-python-network/)\"\n",
    " - Matt Mazur's fantastic article on the theory behind backpropogation, \"[A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\" \n",
    " - Analytics Vidhya's article on \"[Understanding and coding Neural Networks From Scratch in Python and R](https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing the Neural Network\n",
    "Before we can actually do anything, we need to develop the neural network that we will be using. Fortunately I already (roughly) know how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the important packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define any globals.\n",
    "np.random.seed(0)\n",
    "DATA_POINTS = 10000  # 10 Thousand\n",
    "INPUT_NEURONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the important functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Normalises the inputs to be between 0 and 1\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def S(x):\n",
    "    \"\"\"Shorthand notation for the sigmoid function\"\"\"\n",
    "    return sigmoid(x)\n",
    "\n",
    "def dS(x_norm):\n",
    "    \"\"\"Shorthand notation for the derivative of the sigmoid function\"\"\"\n",
    "    return x_norm*(1-x_norm)\n",
    "\n",
    "def errorFunc(expected, actual):\n",
    "    \"\"\"Calculates the error\"\"\"\n",
    "    return 0.5*(expected - actual)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the NN class.\n",
    "class NeuralNetwork(object):\n",
    "    \"\"\"The neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self, x, y, seed=None):\n",
    "        \"\"\"Initialise the NN\"\"\"        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        if seed: np.random.seed(0)\n",
    "        \n",
    "        self.weights = 2*np.random.random((3,1)) - 1\n",
    "        self.bias = 2*np.random.random((1, 1)) - 1\n",
    "        return None\n",
    "    \n",
    "    def train(self, epochs=1, verbose=False):\n",
    "        \"\"\"Trains the network for the given iterations\"\"\"        \n",
    "        for i in xrange(epochs):  # Go over the data N times\n",
    "            if verbose: print \"%i/%i epochs complete\" % (i, epochs)\n",
    "            for x, y in zip(self.x, self.y):  # The data itself (to avoid matrix mechanics)\n",
    "                # Forward propagation\n",
    "                output = S(np.dot(x.T, self.weights) + self.bias[0])\n",
    "\n",
    "                # Calculate error\n",
    "                error = errorFunc(y, output)\n",
    "\n",
    "                # Backward propagate weights\n",
    "                learning_rate = error\n",
    "                delta = dS(output)*(output-y)\n",
    "                self.weights = self.weights + np.array([(-delta*learning_rate)[0]*x]).T\n",
    "                self.bias = self.bias - [delta*learning_rate]  # Since all bias \"inputs\" are 1\n",
    "        \n",
    "        if verbose:\n",
    "            print \"Weights:\", list(self.weights)\n",
    "            print \"Bias:\", list(self.bias)\n",
    "        return None\n",
    "    \n",
    "    def think(self, x):\n",
    "        \"\"\"Given the inputs x, let the NN predict y.\"\"\"\n",
    "        # Forward propagation\n",
    "        input_layer = x\n",
    "        output_layer = S(np.dot(input_layer, self.weights) + self.bias[0])\n",
    "        return output_layer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - The AND gate\n",
    "The first problem I'll be attempting to solve is for the AND gate. This gate takes boolean inputs (True or False) and outputs True if all inputs are True or False if any inputs are False. To express this in Boolean algebra:\n",
    "\n",
    "> AND(A, B, C) = ABC\n",
    "\n",
    "where the inputs can only be 0 or 1. Intuatively I expect that, without normalisation, the weights $w_1$, $w_2$ and $w_3$ will all be 0.33 and the bias, $b$, will be -0.16. That's because the weighted summation will then only be above 0.5 (the \"True\" threshold) when all three inputs fire.\n",
    "\n",
    "With this in mind, let's now generate the data for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AND(x, y, z):\n",
    "    return x and y and z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.randint(2, size=(INPUT_NEURONS*DATA_POINTS))\n",
    "x1 = np.split(x1, DATA_POINTS)\n",
    "\n",
    "y1 = [AND(a, b, c) for a, b, c in x1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data fully primed and ready, we can now train the neural network and see how good of a result it gets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork(x1, y1, seed=1)\n",
    "NN.train(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that the model works, we can test it against the various inputs of the AND gate to see what it thinks the answer might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY CHECK\n",
      "==================\n",
      "[0, 0, 0]: 2.41194618326e-06 -> 0\n",
      "[0, 0, 1]: 0.000405165926404 -> 0\n",
      "[0, 1, 0]: 0.000403058006319 -> 0\n",
      "[0, 1, 1]: 0.0634612518911 -> 0\n",
      "[1, 0, 0]: 0.000412239702661 -> 0\n",
      "[1, 0, 1]: 0.0648137640716 -> 0\n",
      "[1, 1, 0]: 0.0644981850056 -> 0\n",
      "[1, 1, 1]: 0.920548253539 -> 1\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "validation_x = [\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 1],\n",
    "]\n",
    "\n",
    "validation_y = [AND(a, b, c) for a, b, c in validation_x]\n",
    "\n",
    "# Now see how accurate it is.\n",
    "count = 0\n",
    "print \"ACCURACY CHECK\"\n",
    "print \"==================\"\n",
    "for x, y in zip(validation_x, validation_y):\n",
    "    prediction = NN.think(x)\n",
    "    print \"%s: %s -> %i\" % (x, prediction, int(round(prediction)))\n",
    "    count += int(int(round(prediction) == y))\n",
    "print \"\\nAccuracy:\", float(count)/len(validation_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - The OR Gate\n",
    "With the AND gate all done and dusted, we can shift our attention to getting the OR gate data to play around with. The OR gate returns True if any of the inputs are True. You can implement it in Boolean algebra like so:\n",
    "\n",
    "> OR(A, B, C) = A + B + C\n",
    "\n",
    "In this case I expect that the bias should start negative, but the weights on the input neurons will be orders of magnitude greater than the bias's. We can test that theory out quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OR(x, y, z):\n",
    "    return x or y or z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.random.randint(2, size=(INPUT_NEURONS*DATA_POINTS))\n",
    "x2 = np.split(x2, DATA_POINTS)\n",
    "\n",
    "y2 = [OR(a, b, c) for a, b, c in x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = NeuralNetwork(x2, y2, seed=1)\n",
    "NN.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY CHECK\n",
      "==================\n",
      "[0, 0, 0]: 0.0639726575546 -> 0\n",
      "[0, 0, 1]: 0.953637848106 -> 1\n",
      "[0, 1, 0]: 0.953437384377 -> 1\n",
      "[0, 1, 1]: 0.999837758972 -> 1\n",
      "[1, 0, 0]: 0.953964244169 -> 1\n",
      "[1, 0, 1]: 0.999839683023 -> 1\n",
      "[1, 1, 0]: 0.999838956098 -> 1\n",
      "[1, 1, 1]: 0.99999946482 -> 1\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "validation_x = [\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 1],\n",
    "]\n",
    "\n",
    "validation_y = [OR(a, b, c) for a, b, c in validation_x]\n",
    "\n",
    "# Now see how accurate it is.\n",
    "count = 0\n",
    "print \"ACCURACY CHECK\"\n",
    "print \"==================\"\n",
    "for x, y in zip(validation_x, validation_y):\n",
    "    prediction = NN.think(x)\n",
    "    print \"%s: %s -> %i\" % (x, prediction, int(round(prediction)))\n",
    "    count += int(int(round(prediction) == y))\n",
    "print \"\\nAccuracy:\", float(count)/len(validation_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "As you can see, a __very__ simple neural network can be implemented in a very simple manner and you can teach it to understand AND and OR gates with minimal effort. This however is exactly that: simple. It can't predict what image it is looking at, nor can it predict more complex gates like XOR. That might take me a while longer to understand and be able to write. \n",
    "\n",
    "Hopefully my NN has been somewhat useful and interesting to read. Please send me a message if you have any improvements to this notebook or y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
